{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 15:55:49.227881: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-17 15:55:49.229982: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-17 15:55:49.235026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731876949.243676 2880893 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731876949.246045 2880893 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 15:55:49.255899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/giorgio6846/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/giorgio6846/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/giorgio6846/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/giorgio6846/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('corpus')\n",
    "\n",
    "# Configurar visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los csv EDA\n",
    "subreddit_data = pd.read_csv('../data/subreddit_data.csv')\n",
    "posts_data = pd.read_csv('../data/posts_data.csv')\n",
    "subreddit_data = subreddit_data.fillna(\"\")\n",
    "posts_data = posts_data.fillna(\"\")\n",
    "combined_data = posts_data.merge(subreddit_data, on='subreddit_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizacion y Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditTextProcessor:\n",
    "    def __init__(self):\n",
    "        # Inicializar el lematizador\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.columns_to_process = [\"titlePost\", \"tittleSubreddit\", \"descriptionReddit\"]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenización\n",
    "        text = str(text)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Lematización y filtrado de stopwords\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(token) for token in tokens \n",
    "            if token.isalpha() and token not in self.stop_words\n",
    "        ]\n",
    "        # Se devuelve una cadena y no una lista de tokens para los modelos de aprendizaje\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def process_dataframe(self, df):\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == object and column  in self.columns_to_process:\n",
    "                df[column] = df[column].apply(self.preprocess_text)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textProcessor = RedditTextProcessor()\n",
    "processed_data = textProcessor.process_dataframe(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditClustering:\n",
    "    def __init__(self, processed_data):\n",
    "        #Transformacion de texto a valor numerica para procesar datos\n",
    "        self.vectorizer_title = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "        self.vectorizer_subreddit = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "        self.vectorizer_description = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "\n",
    "        tfidf_title = self.vectorizer_title.fit_transform(processed_data['titlePost'])\n",
    "        tfidf_subreddit = self.vectorizer_subreddit.fit_transform(processed_data['tittleSubreddit'])\n",
    "        tfidf_description = self.vectorizer_description.fit_transform(processed_data['descriptionReddit'])\n",
    "\n",
    "        combined_tfidf = hstack([tfidf_title, tfidf_subreddit, tfidf_description])\n",
    "\n",
    "        post_numeric_features  = StandardScaler().fit_transform(processed_data[['upVotes', 'scorePost', 'commentsPost']])\n",
    "        self.combined_features = hstack([combined_tfidf, post_numeric_features])\n",
    "        \n",
    "        \n",
    "    def get_k_values(self):\n",
    "        distortions = []\n",
    "        silhouette_scores = []\n",
    "        K = range(2, 10)\n",
    "        for k in K:\n",
    "            kmeans_model = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans_model.fit(self.combined_features)\n",
    "            distortions.append(kmeans_model.inertia_)\n",
    "            silhouette_avg = silhouette_score(self.combined_features, kmeans_model.labels_)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            print(f\"Para k={k}, el coeficiente de silueta es {silhouette_avg}\")\n",
    "\n",
    "        # Visualización del método del codo\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(K, distortions, 'bx-')\n",
    "        plt.xlabel('Número de clusters')\n",
    "        plt.ylabel('Distorsión')\n",
    "        plt.title('Método del Codo para K óptimo')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualización del coeficiente de silueta\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(K, silhouette_scores, 'bx-')\n",
    "        plt.xlabel('Número de clusters')\n",
    "        plt.ylabel('Coeficiente de Silueta')\n",
    "        plt.title('Coeficiente de Silueta para K óptimo')\n",
    "        plt.show()\n",
    "\n",
    "    def apply_kmeans(self, k, df):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(self.combined_features)\n",
    "        clusters = kmeans.labels_\n",
    "        df['Cluster'] = clusters\n",
    "        score = silhouette_score(self.combined_features, clusters)\n",
    "        print(\"Silhouette Score para clusters:\", score)\n",
    "        return df\n",
    "    def analyze_clusters(self, df, n_keywords=5):\n",
    "        cluster_names = {}\n",
    "        \n",
    "        for cluster_id in sorted(df['Cluster'].unique()):\n",
    "            titles_in_cluster = df[df['Cluster'] == cluster_id]['titlePost']\n",
    "            \n",
    "            tfidf_matrix = self.vectorizer_title.transform(titles_in_cluster)\n",
    "            sum_tfidf = tfidf_matrix.sum(axis=0)\n",
    "            keywords = [(self.vectorizer_title.get_feature_names_out()[i], sum_tfidf[0, i]) \n",
    "                        for i in range(sum_tfidf.shape[1])]\n",
    "            \n",
    "            sorted_keywords = sorted(keywords, key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "            top_keywords = [word for word, score in sorted_keywords]\n",
    "            \n",
    "            cluster_name = \" / \".join(top_keywords)\n",
    "            cluster_names[cluster_id] = cluster_name\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {cluster_name}\")\n",
    "\n",
    "        # Asignar nombres a los clusters en el DataFrame\n",
    "        df['Cluster_Name'] = df['Cluster'].map(cluster_names).astype('category')\n",
    "        return df\n",
    "    def assign_subreddit_names(self, df):\n",
    "        cluster_names = {}\n",
    "\n",
    "        for cluster_id in sorted(df['Cluster'].unique()):\n",
    "            subreddits_in_cluster = df[df['Cluster'] == cluster_id]['tittleSubreddit']\n",
    "            \n",
    "            most_common_subreddit = subreddits_in_cluster.mode()[0]\n",
    "            \n",
    "            cluster_names[cluster_id] = most_common_subreddit\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {most_common_subreddit}\")\n",
    "\n",
    "        df['Cluster_Name'] = df['Cluster'].map(cluster_names).astype('category')\n",
    "        return df\n",
    "\n",
    "\n",
    "clustering = RedditClustering(processed_data)\n",
    "#clustering.get_k_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score para clusters: 0.16628242772450455\n",
      "Cluster 0: ai / chatgpt / asked / real / facebook\n",
      "Cluster 1: insult / way / bro / tho / unsee\n",
      "Cluster 2: mc / main / character / guy / prank\n",
      "Cluster 3: russian / ukrainian / ukraine / soldier / russia\n",
      "Cluster 4: homdgcat / character / new / banner / beta\n",
      "Cluster 5: help / explain / joke / confused / mean\n",
      "Cluster 6: megathread / general / discussion / question / president\n",
      "Cluster 7: trump / harris / kamala / donald / president\n",
      "Cluster 8: insult / like / look / rare / think\n",
      "Cluster 9: trump / donald / harris / biden / republican\n",
      "Cluster 10: aita / say / cat / trump / year\n",
      "Cluster 11: year / like / game / new / day\n",
      "Cluster 12: aitah / people / aita / wife / husband\n",
      "Cluster 13: aita / aitah / people / telling / wife\n",
      "Cluster 14: meirl / year / dad / trump / day\n",
      "Cluster 15: say / harris / kamala / star / trump\n",
      "Cluster 16: food / cooking / dish / cook / recipe\n",
      "Cluster 17: trump / donald / pennsylvania / biden / presidential\n",
      "Cluster 18: germany / german / mean / help / know\n",
      "Cluster 19: roommate / room / update / roomate / bad\n",
      "Cluster 20: amd / ryzen / radeon / rx / cpu\n",
      "Cluster 21: meirl / year / old / know / people\n",
      "Cluster 22: season / diablo / blizzard / drop / game\n",
      "Cluster 23: build / mode / honor / class / act\n",
      "Cluster 24: mod / minecraft / modpack / modded / like\n",
      "Cluster 25: dota / ti / valve / game / patch\n",
      "Cluster 26: meirl / year / man / guy / like\n",
      "Cluster 27: spoiler / wwe / raw / cody / aew\n",
      "Cluster 28: nijisanji / pomu / elira / selen / niji\n",
      "Cluster 29: twitch / stream / streamer / viewer / streaming\n",
      "Cluster 30: like / time / better / know / game\n",
      "Cluster 31: job / interview / recruiter / got / offer\n",
      "Cluster 32: game / steam / dev / gamedev / engine\n",
      "Cluster 33: make / steal / hide / look / play\n",
      "Cluster 34: petah / peter / like / year / cat\n",
      "Cluster 35: megathread / people / general / discussion / question\n",
      "Cluster 36: meirl / oc / til / time / like\n",
      "Cluster 37: aita / game / say / people / telling\n",
      "Cluster 38: server / self / alternative / service / dashboard\n",
      "Cluster 39: warframe / prime / new / guy / got\n",
      "Cluster 40: rtx / super / nvidia / geforce / ti\n",
      "Cluster 41: board / snowboard / day / season / park\n",
      "Cluster 42: car / money / credit / year / house\n",
      "Cluster 43: sukuna / gege / jjk / gojo / chapter\n",
      "Cluster 44: league / ggg / poe / necropolis / exile\n",
      "Cluster 45: job / tech / company / engineer / got\n",
      "Cluster 46: men / woman / guy / girl / dating\n",
      "Cluster 47: meirl / year / man / til / guy\n",
      "Cluster 48: truck / driver / trucker / trucking / got\n",
      "Cluster 49: update / aita / husband / wife / new\n",
      "Cluster 50: meirl / work / photo / trump / man\n",
      "Cluster 51: nopixel / onx / yuno / pd / lang\n",
      "Cluster 52: piracy / pirate / youtube / game / movie\n",
      "Cluster 53: ukraine / russian / ukrainian / russia / drone\n",
      "Cluster 54: ye / kanye / vulture / album / song\n",
      "Cluster 55: video / trailer / year / official / scene\n",
      "Cluster 56: apex / game / legend / respawn / skin\n",
      "Cluster 57: persona / aigis / reload / art / yukari\n",
      "Cluster 58: dawntrail / cosplay / ffxiv / art / new\n",
      "Cluster 59: quest / vr / meta / game / reality\n",
      "Cluster 60: people / american / war / country / century\n",
      "Cluster 61: game / denuvo / pirate / piracy / fitgirl\n",
      "Cluster 62: going / deal / people / talking / whats\n",
      "Cluster 63: apple / iphone / pro / io / vision\n",
      "Cluster 64: game / playstation / say / astro / bot\n",
      "Cluster 65: india / indian / modi / woman / delhi\n",
      "Cluster 66: lpt / use / make / time / buy\n",
      "Cluster 67: guide / cool / world / different / type\n",
      "Cluster 68: sysadmin / user / microsoft / crowdstrike / company\n",
      "Cluster 69: iphone / apple / pro / phone / camera\n",
      "Cluster 70: scam / scammed / got / scammer / phone\n",
      "Cluster 71: wrong / aiw / husband / friend / telling\n",
      "Cluster 72: tifu / telling / getting / wife / accidentally\n",
      "Cluster 73: british / uk / today / morning / local\n",
      "Cluster 74: sa / na / ang / ng / philippine\n",
      "Cluster 75: programming / learn / code / coding / learning\n",
      "Cluster 76: skyrim / game / thing / know / think\n",
      "Cluster 77: vtuber / vtubers / nijisanji / dokibird / doki\n",
      "Cluster 78: pc / build / gaming / gpu / cpu\n",
      "Cluster 79: people / sex / woman / men / guy\n",
      "Cluster 80: man / car / guy / crash / road\n",
      "Cluster 81: uk / people / thing / british / think\n",
      "Cluster 82: flux / ai / diffusion / image / stable\n",
      "Cluster 83: gaijin / tank / vehicle / thunder / game\n",
      "Cluster 84: fan / goal / league / manchester / player\n",
      "Cluster 85: woman / men / husband / abortion / man\n",
      "Cluster 86: russian / soldier / drone / ukrainian / fpv\n",
      "Cluster 87: homelab / rack / server / lab / home\n",
      "Cluster 88: meirl / til / year / man / oc\n",
      "Cluster 89: disc / chapter / ch / oneshot / manga\n",
      "Cluster 90: frugal / money / buy / food / thing\n",
      "Cluster 91: husband / boyfriend / wife / friend / sister\n",
      "Cluster 92: book / read / reading / author / library\n",
      "Cluster 93: house / neighbor / want / child / property\n",
      "Cluster 94: gb / fitgirl / repack / dlcs / repacks\n",
      "Cluster 95: poor / food / money / grocery / poverty\n",
      "Cluster 96: australia / australian / cole / aussie / price\n",
      "Cluster 97: mod / skyrim / release / follower / modding\n",
      "Cluster 98: stelle / character / hsr / art / star\n",
      "Cluster 99: discussion / anime / episode / season / visual\n",
      "Cluster 100: gamers / woke / game / gamer / like\n",
      "Cluster 101: diy / wall / built / bathroom / kitchen\n",
      "Cluster 102: request / true / actually / math / accurate\n",
      "Cluster 103: stardew / valley / ca / farm / game\n",
      "Cluster 104: trip / travel / day / traveling / country\n",
      "Cluster 105: like / game / new / got / year\n",
      "Cluster 106: game / trailer / xbox / steam / say\n",
      "Cluster 107: sod / wow / phase / classic / blizzard\n",
      "Cluster 108: dsk / blb / card / otj / mkm\n",
      "Cluster 109: ame / hololive / biboo / kiara / stream\n",
      "Cluster 110: tekken / character / reina / kazuya / main\n",
      "Cluster 111: cosplay / piece / nami / luffy / robin\n",
      "Cluster 112: man / guy / woman / police / insane\n",
      "Cluster 113: trump / people / say / biden / harris\n",
      "Cluster 114: anime / know / frieren / girl / peak\n",
      "Cluster 115: deck / steam / game / oled / steamdeck\n",
      "Cluster 116: dog / amazing / man / year / art\n",
      "Cluster 117: dlc / elden / ring / bos / game\n",
      "Cluster 118: helldivers / new / democracy / order / game\n",
      "Cluster 119: like / tumblr / thing / real / work\n",
      "        Cluster                                       Cluster_Name\n",
      "0            10                    aita / say / cat / trump / year\n",
      "1            12             aitah / people / aita / wife / husband\n",
      "2            37               aita / game / say / people / telling\n",
      "4           105                     like / game / new / got / year\n",
      "913         113              trump / people / say / biden / harris\n",
      "...         ...                                                ...\n",
      "177151       38  server / self / alternative / service / dashboard\n",
      "178057       51                   nopixel / onx / yuno / pd / lang\n",
      "179052       87               homelab / rack / server / lab / home\n",
      "179941       74                    sa / na / ang / ng / philippine\n",
      "180875       65              india / indian / modi / woman / delhi\n",
      "\n",
      "[120 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "processed_data = clustering.apply_kmeans(120, processed_data)\n",
    "processed_data = clustering.analyze_clusters(processed_data)\n",
    "print(processed_data[['Cluster', 'Cluster_Name']].drop_duplicates())\n",
    "processed_data.to_csv('../data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>titlePost</th>\n",
       "      <th>createdPost</th>\n",
       "      <th>scorePost</th>\n",
       "      <th>upVotedRatio</th>\n",
       "      <th>upVotes</th>\n",
       "      <th>commentsPost</th>\n",
       "      <th>tittleSubreddit</th>\n",
       "      <th>subscribersReddit</th>\n",
       "      <th>descriptionReddit</th>\n",
       "      <th>createdReddit</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Cluster_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mortgage rate</td>\n",
       "      <td>2024-04-24 04:02:06</td>\n",
       "      <td>22162</td>\n",
       "      <td>0.97</td>\n",
       "      <td>22162</td>\n",
       "      <td>1655</td>\n",
       "      <td>home</td>\n",
       "      <td>256578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>10</td>\n",
       "      <td>aita / say / cat / trump / year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>parent bought house month ago company flipped ...</td>\n",
       "      <td>2024-05-17 21:16:37</td>\n",
       "      <td>19061</td>\n",
       "      <td>0.89</td>\n",
       "      <td>19061</td>\n",
       "      <td>5813</td>\n",
       "      <td>home</td>\n",
       "      <td>256578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>12</td>\n",
       "      <td>aitah / people / aita / wife / husband</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>stuff coming ball</td>\n",
       "      <td>2024-10-03 04:06:19</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.95</td>\n",
       "      <td>8139</td>\n",
       "      <td>1643</td>\n",
       "      <td>home</td>\n",
       "      <td>256578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>37</td>\n",
       "      <td>aita / game / say / people / telling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>pella window would ok reacting</td>\n",
       "      <td>2024-08-27 20:18:19</td>\n",
       "      <td>3537</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3537</td>\n",
       "      <td>1739</td>\n",
       "      <td>home</td>\n",
       "      <td>256578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>37</td>\n",
       "      <td>aita / game / say / people / telling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>basement floor leak</td>\n",
       "      <td>2024-07-16 17:54:12</td>\n",
       "      <td>1870</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1870</td>\n",
       "      <td>569</td>\n",
       "      <td>home</td>\n",
       "      <td>256578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>105</td>\n",
       "      <td>like / game / new / got / year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id                                          titlePost  \\\n",
       "0             0                                      mortgage rate   \n",
       "1             0  parent bought house month ago company flipped ...   \n",
       "2             0                                  stuff coming ball   \n",
       "3             0                     pella window would ok reacting   \n",
       "4             0                                basement floor leak   \n",
       "\n",
       "           createdPost  scorePost  upVotedRatio  upVotes  commentsPost  \\\n",
       "0  2024-04-24 04:02:06      22162          0.97    22162          1655   \n",
       "1  2024-05-17 21:16:37      19061          0.89    19061          5813   \n",
       "2  2024-10-03 04:06:19       8139          0.95     8139          1643   \n",
       "3  2024-08-27 20:18:19       3537          0.98     3537          1739   \n",
       "4  2024-07-16 17:54:12       1870          0.98     1870           569   \n",
       "\n",
       "  tittleSubreddit  subscribersReddit descriptionReddit        createdReddit  \\\n",
       "0            home             256578               NaN  2009-01-25 02:25:57   \n",
       "1            home             256578               NaN  2009-01-25 02:25:57   \n",
       "2            home             256578               NaN  2009-01-25 02:25:57   \n",
       "3            home             256578               NaN  2009-01-25 02:25:57   \n",
       "4            home             256578               NaN  2009-01-25 02:25:57   \n",
       "\n",
       "   Cluster                            Cluster_Name  \n",
       "0       10         aita / say / cat / trump / year  \n",
       "1       12  aitah / people / aita / wife / husband  \n",
       "2       37    aita / game / say / people / telling  \n",
       "3       37    aita / game / say / people / telling  \n",
       "4      105          like / game / new / got / year  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = pd.read_csv('../data/processed_data.csv')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_reddit_trends(df):\n",
    "    \"\"\"\n",
    "    Analiza y predice tendencias en datos de Reddit usando series temporales.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame con las columnas createdPost, Cluster, scorePost, upVotes, commentsPost\n",
    "    \n",
    "    Returns:\n",
    "    dict con resultados del análisis y predicciones\n",
    "    \"\"\"\n",
    "    # Convertir la fecha a datetime si no lo está\n",
    "    df['createdPost'] = pd.to_datetime(df['createdPost'])\n",
    "    \n",
    "    # Crear características agregadas por día y cluster\n",
    "    daily_metrics = df.groupby([pd.Grouper(key='createdPost', freq='D'), 'Cluster']).agg({\n",
    "        'scorePost': 'mean',\n",
    "        'upVotes': 'sum',\n",
    "        'commentsPost': 'sum',\n",
    "        'Cluster': 'count'  # Cuenta de posts por cluster\n",
    "    }).rename(columns={'Cluster': 'post_count'})\n",
    "    \n",
    "    # Crear un score de engagement\n",
    "    daily_metrics['engagement_score'] = (\n",
    "        StandardScaler().fit_transform(daily_metrics[['scorePost']]) * 0.3 +\n",
    "        StandardScaler().fit_transform(daily_metrics[['upVotes']]) * 0.4 +\n",
    "        StandardScaler().fit_transform(daily_metrics[['commentsPost']]) * 0.3\n",
    "    )\n",
    "    \n",
    "    # Identificar tendencias actuales\n",
    "    recent_trends = identify_current_trends(daily_metrics)\n",
    "    \n",
    "    # Predecir tendencias futuras\n",
    "    future_trends = predict_future_trends(daily_metrics)\n",
    "    \n",
    "    return {\n",
    "        'current_trends': recent_trends,\n",
    "        'future_predictions': future_trends\n",
    "    }\n",
    "\n",
    "def identify_current_trends(daily_metrics):\n",
    "    \"\"\"\n",
    "    Identifica las tendencias actuales basadas en el engagement reciente\n",
    "    \"\"\"\n",
    "    # Obtener los últimos 30 días de datos\n",
    "    last_date = daily_metrics.index.get_level_values(0).max()\n",
    "    start_date = last_date - timedelta(days=30)\n",
    "    \n",
    "    recent_data = daily_metrics.loc[start_date:last_date]\n",
    "    \n",
    "    # Calcular el promedio de engagement por cluster\n",
    "    cluster_trends = recent_data.groupby(level=1)['engagement_score'].mean()\n",
    "    \n",
    "    # Identificar los clusters más relevantes\n",
    "    top_clusters = cluster_trends.nlargest(5)\n",
    "    \n",
    "    return {\n",
    "        'top_clusters': top_clusters.to_dict(),\n",
    "        'trend_period': f\"{start_date.date()} to {last_date.date()}\"\n",
    "    }\n",
    "\n",
    "def predict_future_trends(daily_metrics):\n",
    "    \"\"\"\n",
    "    Predice tendencias futuras usando modelo Holt-Winters\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Para cada cluster, crear una predicción\n",
    "    for cluster in daily_metrics.index.get_level_values(1).unique():\n",
    "        # Obtener datos del cluster\n",
    "        cluster_data = daily_metrics.xs(cluster, level=1)['engagement_score']\n",
    "        \n",
    "        # Aplicar modelo Holt-Winters si hay suficientes datos\n",
    "        if len(cluster_data) >= 14:  # Mínimo 2 semanas de datos\n",
    "            model = ExponentialSmoothing(\n",
    "                cluster_data,\n",
    "                seasonal_periods=7,  # Patrón semanal\n",
    "                trend='add',\n",
    "                seasonal='add'\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                fitted_model = model.fit()\n",
    "                # Predecir próximos 14 días\n",
    "                forecast = fitted_model.forecast(14)\n",
    "                \n",
    "                # Calcular tendencia\n",
    "                current_avg = cluster_data[-7:].mean()  # Último promedio semanal\n",
    "                predicted_avg = forecast.mean()  # Promedio predicho\n",
    "                trend_direction = \"up\" if predicted_avg > current_avg else \"down\"\n",
    "                \n",
    "                predictions[cluster] = {\n",
    "                    'trend_direction': trend_direction,\n",
    "                    'trend_strength': abs(predicted_avg - current_avg),\n",
    "                    'forecast_values': forecast.to_dict()\n",
    "                }\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def get_trend_insights(results, cluster_names):\n",
    "    \"\"\"\n",
    "    Genera insights legibles sobre las tendencias\n",
    "    \"\"\"\n",
    "    insights = {\n",
    "        'current_trends': [],\n",
    "        'future_predictions': []\n",
    "    }\n",
    "    \n",
    "    # Analizar tendencias actuales\n",
    "    for cluster, score in results['current_trends']['top_clusters'].items():\n",
    "        insights['current_trends'].append({\n",
    "            'cluster': cluster_names.get(cluster, f\"Cluster {cluster}\"),\n",
    "            'engagement_level': 'Alto' if score > 0.5 else 'Medio' if score > 0 else 'Bajo'\n",
    "        })\n",
    "    \n",
    "    # Analizar predicciones\n",
    "    for cluster, pred in results['future_predictions'].items():\n",
    "        if pred['trend_strength'] > 0.5:  # Solo reportar cambios significativos\n",
    "            insights['future_predictions'].append({\n",
    "                'cluster': cluster_names.get(cluster, f\"Cluster {cluster}\"),\n",
    "                'prediction': 'Aumentará' if pred['trend_direction'] == 'up' else 'Disminuirá',\n",
    "                'confidence': 'Alta' if pred['trend_strength'] > 1 else 'Media'\n",
    "            })\n",
    "    \n",
    "    return insights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tendencias Actuales:\n",
      "- meirl / year / man / til / guy: Nivel de engagement Alto\n",
      "- meirl / work / photo / trump / man: Nivel de engagement Alto\n",
      "- meirl / til / year / man / oc: Nivel de engagement Alto\n",
      "- meirl / year / man / guy / like: Nivel de engagement Alto\n",
      "- meirl / year / old / know / people: Nivel de engagement Alto\n",
      "\n",
      "Predicciones:\n"
     ]
    }
   ],
   "source": [
    "cluster_names = processed_data.set_index('Cluster')['Cluster_Name'].to_dict()\n",
    "results = analyze_reddit_trends(processed_data)\n",
    "insights = get_trend_insights(results, cluster_names)\n",
    "\n",
    "\n",
    "print(\"Tendencias Actuales:\")\n",
    "for trend in insights['current_trends']:\n",
    "    print(f\"- {trend['cluster']}: Nivel de engagement {trend['engagement_level']}\")\n",
    "\n",
    "print(\"\\nPredicciones:\")\n",
    "for pred in insights['future_predictions']:\n",
    "    print(f\"- {pred['cluster']}: {pred['prediction']} (Confianza: {pred['confidence']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFAppDaSc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\mathi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\mathi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('corpus')\n",
    "\n",
    "# Configurar visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los csv EDA\n",
    "subreddit_data = pd.read_csv('../data/subreddit_data.csv')\n",
    "posts_data = pd.read_csv('../data/posts_data.csv')\n",
    "subreddit_data = subreddit_data.fillna(\"\")\n",
    "posts_data = posts_data.fillna(\"\")\n",
    "combined_data = posts_data.merge(subreddit_data, on='subreddit_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizacion y Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditTextProcessor:\n",
    "    def __init__(self):\n",
    "        # Inicializar el lematizador\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.columns_to_process = [\"titlePost\", \"tittleSubreddit\", \"descriptionReddit\"]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenización\n",
    "        text = str(text)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        # Lematización y filtrado de stopwords\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(token) for token in tokens \n",
    "            if token.isalpha() and token not in self.stop_words\n",
    "        ]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def process_dataframe(self, df):\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == object and column  in self.columns_to_process:\n",
    "                df[column] = df[column].apply(self.preprocess_text)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textProcessor = RedditTextProcessor()\n",
    "processed_data = textProcessor.process_dataframe(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditClustering:\n",
    "    def __init__(self, processed_data):\n",
    "        #Transformacion de texto a valor numerica para procesar datos\n",
    "        self.vectorizer_title = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "        self.vectorizer_subreddit = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "        self.vectorizer_description = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "\n",
    "        tfidf_title = self.vectorizer_title.fit_transform(processed_data['titlePost'])\n",
    "        tfidf_subreddit = self.vectorizer_subreddit.fit_transform(processed_data['tittleSubreddit'])\n",
    "        tfidf_description = self.vectorizer_description.fit_transform(processed_data['descriptionReddit'])\n",
    "\n",
    "        combined_tfidf = hstack([tfidf_title, tfidf_subreddit, tfidf_description])\n",
    "\n",
    "        post_numeric_features  = StandardScaler().fit_transform(processed_data[['upVotes', 'scorePost', 'commentsPost']])\n",
    "        self.combined_features = hstack([combined_tfidf, post_numeric_features])\n",
    "        \n",
    "        \n",
    "    def get_k_values(self):\n",
    "        distortions = []\n",
    "        silhouette_scores = []\n",
    "        K = range(2, 10)\n",
    "        for k in K:\n",
    "            kmeans_model = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans_model.fit(self.combined_features)\n",
    "            distortions.append(kmeans_model.inertia_)\n",
    "            silhouette_avg = silhouette_score(self.combined_features, kmeans_model.labels_)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            print(f\"Para k={k}, el coeficiente de silueta es {silhouette_avg}\")\n",
    "\n",
    "        # Visualización del método del codo\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(K, distortions, 'bx-')\n",
    "        plt.xlabel('Número de clusters')\n",
    "        plt.ylabel('Distorsión')\n",
    "        plt.title('Método del Codo para K óptimo')\n",
    "        plt.show()\n",
    "\n",
    "        # Visualización del coeficiente de silueta\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(K, silhouette_scores, 'bx-')\n",
    "        plt.xlabel('Número de clusters')\n",
    "        plt.ylabel('Coeficiente de Silueta')\n",
    "        plt.title('Coeficiente de Silueta para K óptimo')\n",
    "        plt.show()\n",
    "\n",
    "    def apply_kmeans(self, k, df):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(self.combined_features)\n",
    "        clusters = kmeans.labels_\n",
    "        df['Cluster'] = clusters\n",
    "        score = silhouette_score(self.combined_features, clusters)\n",
    "        print(\"Silhouette Score para clusters:\", score)\n",
    "        return df\n",
    "    def analyze_clusters(self, df, n_keywords=5):\n",
    "        cluster_names = {}\n",
    "        \n",
    "        for cluster_id in sorted(df['Cluster'].unique()):\n",
    "            titles_in_cluster = df[df['Cluster'] == cluster_id]['titlePost']\n",
    "            \n",
    "            tfidf_matrix = self.vectorizer_title.transform(titles_in_cluster)\n",
    "            sum_tfidf = tfidf_matrix.sum(axis=0)\n",
    "            keywords = [(self.vectorizer_title.get_feature_names_out()[i], sum_tfidf[0, i]) \n",
    "                        for i in range(sum_tfidf.shape[1])]\n",
    "            \n",
    "            sorted_keywords = sorted(keywords, key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "            top_keywords = [word for word, score in sorted_keywords]\n",
    "            \n",
    "            cluster_name = \" / \".join(top_keywords)\n",
    "            cluster_names[cluster_id] = cluster_name\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {cluster_name}\")\n",
    "\n",
    "        # Asignar nombres a los clusters en el DataFrame\n",
    "        df['Cluster_Name'] = df['Cluster'].map(cluster_names).astype('category')\n",
    "        return df\n",
    "    def assign_subreddit_names(self, df):\n",
    "        cluster_names = {}\n",
    "\n",
    "        for cluster_id in sorted(df['Cluster'].unique()):\n",
    "            subreddits_in_cluster = df[df['Cluster'] == cluster_id]['tittleSubreddit']\n",
    "            \n",
    "            most_common_subreddit = subreddits_in_cluster.mode()[0]\n",
    "            \n",
    "            cluster_names[cluster_id] = most_common_subreddit\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {most_common_subreddit}\")\n",
    "\n",
    "        df['Cluster_Name'] = df['Cluster'].map(cluster_names).astype('category')\n",
    "        return df\n",
    "\n",
    "\n",
    "clustering = RedditClustering(processed_data)\n",
    "#clustering.get_k_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faboa\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score para clusters: 0.30650164634368676\n",
      "Cluster 0: riot / league / game / faker / lcs\n",
      "Cluster 1: local / card / year / flew / finger\n",
      "Cluster 2: people / guy / reddit / wow / man\n",
      "Cluster 3: game / larian / cosplay / like / gale\n",
      "Cluster 4: people / thing / really / fuck / happened\n",
      "Cluster 5: dice / aita / aitah / piece / approved\n",
      "Cluster 6: trump / republican / biden / donald / say\n",
      "Cluster 7: genshin / cosplay / character / kirara / fontaine\n",
      "Cluster 8: year / day / difference / bear / diver\n",
      "Cluster 9: fix / house / wall / door / home\n",
      "Cluster 10: man / woman / karen / guy / officer\n",
      "Cluster 11: guide / cool / state / type / different\n",
      "Cluster 12: men / woman / guy / girl / relationship\n",
      "Cluster 13: school / america / follower / japan / true\n",
      "Cluster 14: man / water / year / baby / road\n",
      "Cluster 15: osrs / jagex / runescape / like / new\n",
      "Cluster 16: minecraft / world / like / survival / think\n",
      "Cluster 17: petah / peter / help / peetah / explain\n",
      "Cluster 18: cat / transformation / moral / year / reddit\n",
      "Cluster 19: got / paid / ordered / day / doordash\n",
      "Cluster 20: people / weird / like / american / car\n",
      "Cluster 21: game / got / year / home / like\n",
      "Cluster 22: aita / husband / update / oop / wife\n",
      "Cluster 23: want / car / child / daughter / police\n",
      "Cluster 24: dad / boy / day / spez / fan\n",
      "Cluster 25: roommate / update / room / rent / left\n",
      "Cluster 26: order / helldivers / democracy / major / new\n",
      "Cluster 27: xqc / twitch / banned / hasan / stream\n",
      "Cluster 28: russian / ukrainian / soldier / drone / hit\n",
      "Cluster 29: pirate / piracy / youtube / download / guy\n",
      "Cluster 30: discord / server / nitro / change / account\n",
      "Cluster 31: hc / hardcore / sod / raid / blizzard\n",
      "Cluster 32: warframe / new / prime / game / like\n",
      "Cluster 33: request / true / actually / accurate / long\n",
      "Cluster 34: character / stelle / art / silver / wolf\n",
      "Cluster 35: lifeweaver / overwatch / new / skin / hero\n",
      "Cluster 36: disc / oneshot / chapter / ch / horideiyasumi\n",
      "Cluster 37: game / gamers / woke / guy / gamer\n",
      "Cluster 38: mod / modpack / minecraft / create / know\n",
      "Cluster 39: user / company / microsoft / today / new\n",
      "Cluster 40: people / school / need / dating / make\n",
      "Cluster 41: pc / phone / computer / laptop / internet\n",
      "Cluster 42: like / make / year / body / car\n",
      "Cluster 43: credit / bank / pay / account / car\n",
      "Cluster 44: discussion / episode / season / frieren / anime\n",
      "Cluster 45: man / guy / woman / car / gas\n",
      "Cluster 46: people / aita / movie / thing / aitah\n",
      "Cluster 47: chatgpt / ai / gpt / asked / prompt\n",
      "Cluster 48: elden / ring / time / game / dlc\n",
      "Cluster 49: today / photo / year / weekend / picture\n",
      "Cluster 50: tarkov / bsg / scav / new / cheater\n",
      "Cluster 51: youtube / ad / video / premium / like\n",
      "Cluster 52: deck / steam / game / oled / steamdeck\n",
      "Cluster 53: palworld / pal / base / game / like\n",
      "Cluster 54: like / kid / fantasy / ancient / scientist\n",
      "Cluster 55: til / year / time / million / man\n",
      "Cluster 56: going / deal / people / trump / saying\n",
      "Cluster 57: goal / league / messi / fan / inter\n",
      "Cluster 58: husband / boyfriend / wife / girlfriend / want\n",
      "Cluster 59: major / csgo / valve / paris / player\n",
      "Cluster 60: highlight / nfl / source / game / touchdown\n",
      "Cluster 61: stock / elon / nvda / market / month\n",
      "Cluster 62: country / map / europe / state / world\n",
      "Cluster 63: russian / ukrainian / ukraine / soldier / russia\n",
      "Cluster 64: men / oh / kid / man / cringe\n",
      "Cluster 65: say / abortion / court / dy / florida\n",
      "Cluster 66: medieval / guy / like / time / gentry\n",
      "Cluster 67: parent / circa / dad / mom / school\n",
      "Cluster 68: game / cosplay / year / time / gaming\n",
      "Cluster 69: video / fucking / year / youtube / trailer\n",
      "Cluster 70: spez / heil / anon / used / year\n",
      "Cluster 71: art / ffxiv / cosplay / dawntrail / job\n",
      "Cluster 72: aita / telling / wedding / sister / daughter\n",
      "Cluster 73: steam / game / valve / review / new\n",
      "Cluster 74: game / steam / pc / starfield / gate\n",
      "Cluster 75: game / love / cyberpunk / worst / cosplay\n",
      "Cluster 76: amazing / bird / genius / baby / dog\n",
      "Cluster 77: wtf / car / dude / guy / man\n",
      "Cluster 78: got / time / make / need / black\n",
      "Cluster 79: pc / game / gaming / year / window\n",
      "Cluster 80: poster / official / movie / image / film\n",
      "Cluster 81: professor / college / class / student / roommate\n",
      "Cluster 82: game / baldur / steam / gate / trailer\n",
      "Cluster 83: living / apartment / room / space / year\n",
      "Cluster 84: wife / old / neighbor / twin / husband\n",
      "Cluster 85: pc / build / gaming / building / rtx\n",
      "Cluster 86: suck / got / car / night / hour\n",
      "Cluster 87: piece / cosplay / luffy / action / live\n",
      "Cluster 88: celebrity / oscar / year / favourite / tv\n",
      "Cluster 89: alcohol / ruin / drink / fuck / kid\n",
      "Cluster 90: like / cool / mean / eu / way\n",
      "Cluster 91: insult / like / true / heard / way\n",
      "Cluster 92: bathroom / built / basement / diy / house\n",
      "Cluster 93: time / like / fuck / know / reddit\n",
      "Cluster 94: game / fitgirl / piracy / pc / denuvo\n",
      "Cluster 95: year / today / tiny / sign / local\n",
      "Cluster 96: dad / little / year / cat / know\n",
      "Cluster 97: trump / right / reddit / american / say\n",
      "Cluster 98: people / say / america / aita / game\n",
      "Cluster 99: wow / new / year / like / blizzard\n",
      "Cluster 100: say / man / woman / republican / new\n",
      "Cluster 101: dog / man / world / ocean / fort\n",
      "Cluster 102: bro / girl / nice / watch / man\n",
      "Cluster 103: russia / ukraine / russian / say / putin\n",
      "Cluster 104: cyberpunk / update / game / like / time\n",
      "Cluster 105: spez / heil / praise / anon / glory\n",
      "Cluster 106: man / work / day / job / kid\n",
      "Cluster 107: neighbor / wife / went / drunk / need\n",
      "Cluster 108: reddit / user / say / tesla / musk\n",
      "Cluster 109: lol / sure / right / watch / dream\n",
      "Cluster 110: oc / art / player / dm / dice\n",
      "Cluster 111: job / bos / work / interview / got\n",
      "Cluster 112: story / bro / got / way / girl\n",
      "Cluster 113: trump / republican / people / elon / florida\n",
      "Cluster 114: men / woman / thing / guy / advice\n",
      "Cluster 115: american / poor / work / want / need\n",
      "Cluster 116: thing / people / sex / know / men\n",
      "Cluster 117: aitah / aita / telling / wife / husband\n",
      "Cluster 118: max / race / gp / hamilton / ferrari\n",
      "Cluster 119: highlight / nba / game / charania / draymond\n",
      "       Cluster                                      Cluster_Name\n",
      "0            9                  fix / house / wall / door / home\n",
      "300         84            wife / old / neighbor / twin / husband\n",
      "304         97           trump / right / reddit / american / say\n",
      "306         89               alcohol / ruin / drink / fuck / kid\n",
      "307          4         people / thing / really / fuck / happened\n",
      "...        ...                                               ...\n",
      "28500       52            deck / steam / game / oled / steamdeck\n",
      "28800       81  professor / college / class / student / roommate\n",
      "29100       36     disc / oneshot / chapter / ch / horideiyasumi\n",
      "29406       45                     man / guy / woman / car / gas\n",
      "29737       91                insult / like / true / heard / way\n",
      "\n",
      "[120 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "processed_data = clustering.apply_kmeans(120, processed_data)\n",
    "processed_data = clustering.analyze_clusters(processed_data)\n",
    "print(processed_data[['Cluster', 'Cluster_Name']].drop_duplicates())\n",
    "processed_data.to_csv('../data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>titlePost</th>\n",
       "      <th>createdPost</th>\n",
       "      <th>scorePost</th>\n",
       "      <th>upVotedRatio</th>\n",
       "      <th>upVotes</th>\n",
       "      <th>commentsPost</th>\n",
       "      <th>tittleSubreddit</th>\n",
       "      <th>subscribersReddit</th>\n",
       "      <th>descriptionReddit</th>\n",
       "      <th>createdReddit</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Cluster_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>price range house like</td>\n",
       "      <td>2023-08-18 10:17:43</td>\n",
       "      <td>11633</td>\n",
       "      <td>0.92</td>\n",
       "      <td>11633</td>\n",
       "      <td>1727</td>\n",
       "      <td>home</td>\n",
       "      <td>243743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>9</td>\n",
       "      <td>fix / house / wall / door / home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>door garage</td>\n",
       "      <td>2023-05-18 16:41:57</td>\n",
       "      <td>6026</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6026</td>\n",
       "      <td>1700</td>\n",
       "      <td>home</td>\n",
       "      <td>243743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>9</td>\n",
       "      <td>fix / house / wall / door / home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>fix garage door torsion spring</td>\n",
       "      <td>2023-08-26 19:44:33</td>\n",
       "      <td>5581</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5581</td>\n",
       "      <td>3644</td>\n",
       "      <td>home</td>\n",
       "      <td>243743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>9</td>\n",
       "      <td>fix / house / wall / door / home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>call type design</td>\n",
       "      <td>2023-08-21 17:37:07</td>\n",
       "      <td>2824</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2824</td>\n",
       "      <td>579</td>\n",
       "      <td>home</td>\n",
       "      <td>243743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>9</td>\n",
       "      <td>fix / house / wall / door / home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>inspector said termite damage year old house m...</td>\n",
       "      <td>2023-07-28 17:40:05</td>\n",
       "      <td>2692</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2692</td>\n",
       "      <td>952</td>\n",
       "      <td>home</td>\n",
       "      <td>243743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-25 02:25:57</td>\n",
       "      <td>9</td>\n",
       "      <td>fix / house / wall / door / home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_id                                          titlePost  \\\n",
       "0             0                             price range house like   \n",
       "1             0                                        door garage   \n",
       "2             0                     fix garage door torsion spring   \n",
       "3             0                                   call type design   \n",
       "4             0  inspector said termite damage year old house m...   \n",
       "\n",
       "           createdPost  scorePost  upVotedRatio  upVotes  commentsPost  \\\n",
       "0  2023-08-18 10:17:43      11633          0.92    11633          1727   \n",
       "1  2023-05-18 16:41:57       6026          0.96     6026          1700   \n",
       "2  2023-08-26 19:44:33       5581          0.86     5581          3644   \n",
       "3  2023-08-21 17:37:07       2824          0.96     2824           579   \n",
       "4  2023-07-28 17:40:05       2692          0.94     2692           952   \n",
       "\n",
       "  tittleSubreddit  subscribersReddit descriptionReddit        createdReddit  \\\n",
       "0            home             243743               NaN  2009-01-25 02:25:57   \n",
       "1            home             243743               NaN  2009-01-25 02:25:57   \n",
       "2            home             243743               NaN  2009-01-25 02:25:57   \n",
       "3            home             243743               NaN  2009-01-25 02:25:57   \n",
       "4            home             243743               NaN  2009-01-25 02:25:57   \n",
       "\n",
       "   Cluster                      Cluster_Name  \n",
       "0        9  fix / house / wall / door / home  \n",
       "1        9  fix / house / wall / door / home  \n",
       "2        9  fix / house / wall / door / home  \n",
       "3        9  fix / house / wall / door / home  \n",
       "4        9  fix / house / wall / door / home  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = pd.read_csv('../data/processed_data.csv')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_reddit_trends(df):\n",
    "    \"\"\"\n",
    "    Analiza y predice tendencias en datos de Reddit usando series temporales.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame con las columnas createdPost, Cluster, scorePost, upVotes, commentsPost\n",
    "    \n",
    "    Returns:\n",
    "    dict con resultados del análisis y predicciones\n",
    "    \"\"\"\n",
    "    # Convertir la fecha a datetime si no lo está\n",
    "    df['createdPost'] = pd.to_datetime(df['createdPost'])\n",
    "    \n",
    "    # Crear características agregadas por día y cluster\n",
    "    daily_metrics = df.groupby([pd.Grouper(key='createdPost', freq='D'), 'Cluster']).agg({\n",
    "        'scorePost': 'mean',\n",
    "        'upVotes': 'sum',\n",
    "        'commentsPost': 'sum',\n",
    "        'Cluster': 'count'  # Cuenta de posts por cluster\n",
    "    }).rename(columns={'Cluster': 'post_count'})\n",
    "    \n",
    "    # Crear un score de engagement\n",
    "    daily_metrics['engagement_score'] = (\n",
    "        StandardScaler().fit_transform(daily_metrics[['scorePost']]) * 0.3 +\n",
    "        StandardScaler().fit_transform(daily_metrics[['upVotes']]) * 0.4 +\n",
    "        StandardScaler().fit_transform(daily_metrics[['commentsPost']]) * 0.3\n",
    "    )\n",
    "    \n",
    "    # Identificar tendencias actuales\n",
    "    recent_trends = identify_current_trends(daily_metrics)\n",
    "    \n",
    "    # Predecir tendencias futuras\n",
    "    future_trends = predict_future_trends(daily_metrics)\n",
    "    \n",
    "    return {\n",
    "        'current_trends': recent_trends,\n",
    "        'future_predictions': future_trends\n",
    "    }\n",
    "\n",
    "def identify_current_trends(daily_metrics):\n",
    "    \"\"\"\n",
    "    Identifica las tendencias actuales basadas en el engagement reciente\n",
    "    \"\"\"\n",
    "    # Obtener los últimos 30 días de datos\n",
    "    last_date = daily_metrics.index.get_level_values(0).max()\n",
    "    start_date = last_date - timedelta(days=30)\n",
    "    \n",
    "    recent_data = daily_metrics.loc[start_date:last_date]\n",
    "    \n",
    "    # Calcular el promedio de engagement por cluster\n",
    "    cluster_trends = recent_data.groupby(level=1)['engagement_score'].mean()\n",
    "    \n",
    "    # Identificar los clusters más relevantes\n",
    "    top_clusters = cluster_trends.nlargest(5)\n",
    "    \n",
    "    return {\n",
    "        'top_clusters': top_clusters.to_dict(),\n",
    "        'trend_period': f\"{start_date.date()} to {last_date.date()}\"\n",
    "    }\n",
    "\n",
    "def predict_future_trends(daily_metrics):\n",
    "    \"\"\"\n",
    "    Predice tendencias futuras usando modelo Holt-Winters\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Para cada cluster, crear una predicción\n",
    "    for cluster in daily_metrics.index.get_level_values(1).unique():\n",
    "        # Obtener datos del cluster\n",
    "        cluster_data = daily_metrics.xs(cluster, level=1)['engagement_score']\n",
    "        \n",
    "        # Aplicar modelo Holt-Winters si hay suficientes datos\n",
    "        if len(cluster_data) >= 14:  # Mínimo 2 semanas de datos\n",
    "            model = ExponentialSmoothing(\n",
    "                cluster_data,\n",
    "                seasonal_periods=7,  # Patrón semanal\n",
    "                trend='add',\n",
    "                seasonal='add'\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                fitted_model = model.fit()\n",
    "                # Predecir próximos 14 días\n",
    "                forecast = fitted_model.forecast(14)\n",
    "                \n",
    "                # Calcular tendencia\n",
    "                current_avg = cluster_data[-7:].mean()  # Último promedio semanal\n",
    "                predicted_avg = forecast.mean()  # Promedio predicho\n",
    "                trend_direction = \"up\" if predicted_avg > current_avg else \"down\"\n",
    "                \n",
    "                predictions[cluster] = {\n",
    "                    'trend_direction': trend_direction,\n",
    "                    'trend_strength': abs(predicted_avg - current_avg),\n",
    "                    'forecast_values': forecast.to_dict()\n",
    "                }\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def get_trend_insights(results, cluster_names):\n",
    "    \"\"\"\n",
    "    Genera insights legibles sobre las tendencias\n",
    "    \"\"\"\n",
    "    insights = {\n",
    "        'current_trends': [],\n",
    "        'future_predictions': []\n",
    "    }\n",
    "    \n",
    "    # Analizar tendencias actuales\n",
    "    for cluster, score in results['current_trends']['top_clusters'].items():\n",
    "        insights['current_trends'].append({\n",
    "            'cluster': cluster_names.get(cluster, f\"Cluster {cluster}\"),\n",
    "            'engagement_level': 'Alto' if score > 0.5 else 'Medio' if score > 0 else 'Bajo'\n",
    "        })\n",
    "    \n",
    "    # Analizar predicciones\n",
    "    for cluster, pred in results['future_predictions'].items():\n",
    "        if pred['trend_strength'] > 0.5:  # Solo reportar cambios significativos\n",
    "            insights['future_predictions'].append({\n",
    "                'cluster': cluster_names.get(cluster, f\"Cluster {cluster}\"),\n",
    "                'prediction': 'Aumentará' if pred['trend_direction'] == 'up' else 'Disminuirá',\n",
    "                'confidence': 'Alta' if pred['trend_strength'] > 1 else 'Media'\n",
    "            })\n",
    "    \n",
    "    return insights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tendencias Actuales:\n",
      "- school / america / follower / japan / true: Nivel de engagement Alto\n",
      "- cat / transformation / moral / year / reddit: Nivel de engagement Alto\n",
      "- people / guy / reddit / wow / man: Nivel de engagement Alto\n",
      "- wife / old / neighbor / twin / husband: Nivel de engagement Alto\n",
      "- dad / boy / day / spez / fan: Nivel de engagement Alto\n",
      "\n",
      "Predicciones:\n",
      "- people / aita / movie / thing / aitah: Disminuirá (Confianza: Media)\n",
      "- people / say / america / aita / game: Disminuirá (Confianza: Media)\n",
      "- order / helldivers / democracy / major / new: Aumentará (Confianza: Alta)\n"
     ]
    }
   ],
   "source": [
    "cluster_names = processed_data.set_index('Cluster')['Cluster_Name'].to_dict()\n",
    "results = analyze_reddit_trends(processed_data)\n",
    "insights = get_trend_insights(results, cluster_names)\n",
    "\n",
    "\n",
    "print(\"Tendencias Actuales:\")\n",
    "for trend in insights['current_trends']:\n",
    "    print(f\"- {trend['cluster']}: Nivel de engagement {trend['engagement_level']}\")\n",
    "\n",
    "print(\"\\nPredicciones:\")\n",
    "for pred in insights['future_predictions']:\n",
    "    print(f\"- {pred['cluster']}: {pred['prediction']} (Confianza: {pred['confidence']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
